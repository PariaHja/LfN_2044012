{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y2MQqrPLoJnD"
      },
      "outputs": [],
      "source": [
        "#*****************************\n",
        "# nome: Paria\n",
        "# cognome: Haji Abolfath\n",
        "# 2044012\n",
        "#*****************************\n",
        "\n",
        "!pip install torch_geometric\n",
        "# !pip3 install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "uwA3uAbjoMu1"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch_geometric.datasets import TUDataset\n",
        "from torch_geometric.loader import DataLoader \n",
        "# functionals ?? loss funtion\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import pandas as pd\n",
        "from torch_geometric.data import Data\n",
        "from decimal import Decimal\n",
        "from torch.utils.data import RandomSampler, DataLoader, Subset\n",
        "from torch_geometric.transforms import BaseTransform, Compose\n",
        "import os.path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wmYG-rHUhi5v",
        "outputId": "992683f6-ba35-41bf-b1ed-85163a4f3684"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading https://www.chrsmrrs.com/graphkerneldatasets/DHFR.zip\n",
            "Extracting tmp/DHFR/DHFR.zip\n",
            "Processing...\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "dataAlias = 'DHFR'\n",
        "dataset = TUDataset(root=\"/\".join(['.','tmp']), name=dataAlias)\n",
        "\n",
        "# shuffle the whole dataset - IS MOVED to several cells below, because of additional data pre-processing\n",
        "# perm = torch.randperm(len(dataset))\n",
        "# dataset = dataset[perm]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uip33XaI6NsM",
        "outputId": "6c9eec63-eb77-467c-90f2-e2ceed238ed3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "30\n",
            "30\n",
            "7\n",
            "(tensor([7]),)\n",
            "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
            "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
            "torch.Size([32075, 53])\n"
          ]
        }
      ],
      "source": [
        "cte_shape_for_all = ((dataset[0].x).shape)[1] # 53 - one-hot representation of the chemical atom\n",
        "var_shape_among_all = ((dataset[0].x).shape)[0] # the number of nodes in the graph\n",
        "# print(var_shape_among_all)\n",
        "\n",
        "# so we have 53 atoms -\n",
        "# each is represented by a binary num - and thus can be represented by a single integer as well\n",
        "a=(dataset[0]).x\n",
        "# print(a.shape[0])\n",
        "num_of_atoms = 0\n",
        "num_of_tot_atoms = 53\n",
        "graph_int_ = \"\" # an integer representing the atoms in the graph\n",
        "c=0\n",
        "\n",
        "graph_node_count=[]\n",
        "for i in range(len(dataset)):\n",
        "  for j in range(((dataset[i].x).shape)[0]): # iterate over the number of the atoms in graph i\n",
        "      graph_int_ = graph_int_+str(int(((torch.nonzero((dataset[i].x)[j], as_tuple=True))[0])[0]))\n",
        "      c=c+1\n",
        "  graph_node_count.append(c)\n",
        "  c=0\n",
        "  graph_int_ = \"\" # reset for the next graph (idx i)\n",
        "\n",
        "graph_edge_count=[]\n",
        "c=0\n",
        "for i in range(len(dataset)):\n",
        "  for j in range((((dataset[i]).edge_index).shape)[1]):\n",
        "    c=c+1\n",
        "  graph_edge_count.append(c)\n",
        "  c=0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvvwNwzzEAiM"
      },
      "source": [
        "###### each element is a graph. And the descriptive info that comes with it contains:\n",
        "###### 1. edge_index : describes the connections between nodes (edges) , always of dim (2, num_edges); 2 because of bi-directional\n",
        "###### 2. x is a one-hot representation of the nodes in the graph (Node feature matrix with shape [num_nodes, num_node_features])\n",
        "######     ( in the case where we have no features intially, this tensors is majorly filled with 0s, representing the nodes from 0 to num_nodes-1)\n",
        "###### 3. y label of the graph\n",
        "\n",
        "#####https://pytorch-geometric.readthedocs.io/en/latest/generated/torch_geometric.data.Data.html#torch_geometric.data.Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "jyutBgMf2-yW"
      },
      "outputs": [],
      "source": [
        "# degree\n",
        "df = open('texts/DegOut_.txt', \"r\")\n",
        "lines_d = df.readlines()\n",
        "df.close()\n",
        "\n",
        "# remove /n at the end of each line\n",
        "for index, line in enumerate(lines_d):\n",
        "      lines_d[index] = line.strip()\n",
        "\n",
        "# betweenness centrality\n",
        "df = open('texts/BetOut_.txt', \"r\")\n",
        "lines_b = df.readlines()\n",
        "df.close()\n",
        "\n",
        "for index, line in enumerate(lines_b):\n",
        "      lines_b[index] = line.strip()\n",
        "\n",
        "# closeness centrality\n",
        "df = open('texts/ClosOut_.txt', \"r\")\n",
        "lines_c = df.readlines()\n",
        "df.close()\n",
        "\n",
        "for index, line in enumerate(lines_c):\n",
        "      lines_c[index] = line.strip()\n",
        "\n",
        "# graphlets of length three : triangles\n",
        "df = open('texts/graphlet_counts_len_3.txt', \"r\")\n",
        "lines_g3 = df.readlines()\n",
        "df.close()\n",
        "\n",
        "for index, line in enumerate(lines_g3):\n",
        "      lines_g3[index] = line.strip()\n",
        "\n",
        "# graphlets of length four : ??\n",
        "df = open('texts/graphlet_counts_len_4.txt', \"r\")\n",
        "lines_g4 = df.readlines()\n",
        "df.close()\n",
        "\n",
        "for index, line in enumerate(lines_g4):\n",
        "      lines_g4[index] = line.strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "Be5B0D6Dqo78"
      },
      "outputs": [],
      "source": [
        "# convert the features from string to decimal\n",
        "\n",
        "# betweenness centrality\n",
        "lines_b_m = lines_b # 'm' for modified\n",
        "for i in range (len(lines_b)):\n",
        "  lines_b_m[i] = (Decimal(lines_b[i]))\n",
        "\n",
        "# degree\n",
        "lines_d_m = lines_d # 'm' for modified\n",
        "for i in range (len(lines_d)):\n",
        "  lines_d_m[i] = (Decimal(lines_d[i]))\n",
        "\n",
        "# closeness centrality\n",
        "lines_c_m = lines_c # 'm' for modified\n",
        "for i in range (len(lines_c)):\n",
        "  lines_c_m[i] = (Decimal(lines_c[i]))\n",
        "\n",
        "# graphlet len 4\n",
        "lines_g4_m = lines_g4 # 'm' for modified\n",
        "for i in range (len(lines_g4)):\n",
        "  lines_g4_m[i] = (Decimal(lines_g4[i]))\n",
        "\n",
        "# graphlet len 3\n",
        "lines_g3_m = lines_g3 # 'm' for modified\n",
        "for i in range (len(lines_g3)):\n",
        "  lines_g3_m[i] = (Decimal(lines_g3[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9iHUspw_lrTk"
      },
      "outputs": [],
      "source": [
        "# each graph will have 3 new features : avg degree, avg bet, avg clos\n",
        "\n",
        "# *** DEGREE DEGREE ***\n",
        "deg_sum = 0\n",
        "deg_avg_all = []\n",
        "deg_lines_idx = 0\n",
        "\n",
        "for i in range (len(dataset)): # 756 - num of graphs\n",
        "  for j in range((dataset[i]).num_nodes):\n",
        "    deg_sum = deg_sum + lines_d_m[deg_lines_idx]\n",
        "    deg_lines_idx = deg_lines_idx + 1\n",
        "  deg_avg_all.append((deg_sum/(dataset[i]).num_nodes))\n",
        "  deg_sum = 0\n",
        "\n",
        "# *** VARIANCE OF DEGREE *** : (sum(lines_d_m[deg_lines_idx] - deg_avg_all[i])) / ((dataset[i]).num_nodes)\n",
        "sum_of_diffs = 0\n",
        "var_deg_all = []\n",
        "deg_lines_idx = 0\n",
        "\n",
        "for i in range (len(dataset)):\n",
        "  for j in range((dataset[i]).num_nodes):\n",
        "    sum_of_diffs = sum_of_diffs + abs(lines_d_m[deg_lines_idx] - deg_avg_all[i])\n",
        "    deg_lines_idx = deg_lines_idx + 1\n",
        "  var_deg_all.append(((sum_of_diffs) ** 2) / ((dataset[i]).num_nodes) - 1)\n",
        "  sum_of_diffs = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UT4SJtwtrA63"
      },
      "outputs": [],
      "source": [
        "# *** AVERAGE BETWEENNESS CENTRALITY ***\n",
        "bet_sum = 0\n",
        "bet_avg_all = []\n",
        "bet_lines_idx = 0\n",
        "\n",
        "for i in range (len(dataset)):\n",
        "  for j in range((dataset[i]).num_nodes):\n",
        "    bet_sum = bet_sum + lines_b_m[bet_lines_idx]\n",
        "    bet_lines_idx = bet_lines_idx + 1\n",
        "  bet_avg_all.append((bet_sum/(dataset[i]).num_nodes))\n",
        "  bet_sum = 0\n",
        "\n",
        "# *** VARIANCE OF BETWEENNESS CENTRALITY ***\n",
        "sum_of_diffs = 0\n",
        "var_bet_all = []\n",
        "bet_lines_idx = 0\n",
        "\n",
        "for i in range (len(dataset)):\n",
        "  for j in range((dataset[i]).num_nodes):\n",
        "    sum_of_diffs = sum_of_diffs + abs(lines_b_m[bet_lines_idx] - bet_avg_all[i])\n",
        "    bet_lines_idx = bet_lines_idx + 1\n",
        "  var_bet_all.append(((sum_of_diffs) ** 2) / ((dataset[i]).num_nodes) - 1)\n",
        "  sum_of_diffs = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "tv-fNOrssTsv"
      },
      "outputs": [],
      "source": [
        "# *** AVERAGE CLOSENESS CENTRALITY ***\n",
        "close_sum = 0\n",
        "close_avg_all = []\n",
        "close_lines_idx = 0\n",
        "\n",
        "for i in range (len(dataset)):\n",
        "  for j in range((dataset[i]).num_nodes):\n",
        "    close_sum = close_sum + lines_c_m[close_lines_idx]\n",
        "    close_lines_idx = close_lines_idx + 1\n",
        "  close_avg_all.append((close_sum/(dataset[i]).num_nodes))\n",
        "  close_sum = 0\n",
        "\n",
        "# *** VARIANCE OF CLOSENESS CENTRALITY ***\n",
        "sum_of_diffs = 0\n",
        "var_close_all = []\n",
        "close_lines_idx = 0\n",
        "\n",
        "for i in range (len(dataset)):\n",
        "  for j in range((dataset[i]).num_nodes):\n",
        "    sum_of_diffs = sum_of_diffs + abs(lines_c_m[close_lines_idx] - close_avg_all[i])\n",
        "    close_lines_idx = close_lines_idx + 1\n",
        "  var_close_all.append(((sum_of_diffs) ** 2) / ((dataset[i]).num_nodes) - 1)\n",
        "  sum_of_diffs = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vbS5evDvs0Pl"
      },
      "outputs": [],
      "source": [
        "# now we have 3 lists for the averages, those need to be converted to tensors first - that's requierd in order to work with the DL methods\n",
        "\n",
        "# betweennesss centrality\n",
        "torch.set_printoptions(precision=17)\n",
        "torch.set_default_dtype(torch.float32)\n",
        "betweenness_tensor = torch.Tensor(var_bet_all)\n",
        "\n",
        "# closeness centrality - seems to be more useful in discriminating between different graphs\n",
        "torch.set_printoptions(precision=17)\n",
        "torch.set_default_dtype(torch.float32)\n",
        "closeness_tensor = torch.Tensor(var_close_all)\n",
        "\n",
        "# degree\n",
        "torch.set_printoptions(precision=17)\n",
        "torch.set_default_dtype(torch.float32)\n",
        "degree_tensor = torch.Tensor(var_deg_all)\n",
        "\n",
        "# graph node count\n",
        "node_count_tensor = torch.Tensor(graph_node_count)\n",
        "# graph edge count\n",
        "edge_count_tensor = torch.Tensor(graph_edge_count)\n",
        "\n",
        "# graphlet length 3\n",
        "graphlet_three_tensor = torch.Tensor(lines_g3_m)\n",
        "\n",
        "# graphlet length 4\n",
        "graphlet_four_tensor = torch.Tensor(lines_g4_m)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cVkv-zfV5jz",
        "outputId": "80395720-e9b8-4fba-8b58-81df6c9fa6b6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'torch_geometric.datasets.tu_dataset.TUDataset'>\n"
          ]
        }
      ],
      "source": [
        "# logistic regression - solve the issue with one-hot in x - variance - DONE\n",
        "# TUDataset is a Homogeneous dataset - for which it is used a Data which is : A data object describing a homogeneous graph.\n",
        "# a homogeneous dataset contains data of a unique type for instance all are numerical\n",
        "# but in a heterogeneous one, we have data of types numerical, categorical, textual e cosi via.\n",
        "\n",
        "# in HDFR, we have only numerical information so it's a homogeneous one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDedQm9W3Pfe"
      },
      "outputs": [],
      "source": [
        "# save an object to a disk file, tensors and models are saved used the .pt file format\n",
        "\n",
        "# THIS BLOCK IS EXECUTED ONLY ONCE (FROM FOLLOWING EXECUTATIONS, THE TENSORS WILL ONLY BE LOADED)\n",
        "torch.save(degree_tensor, 'tensors/degrees.pt')\n",
        "torch.save(betweenness_tensor, 'tensors/betweenness_centrality.pt')\n",
        "torch.save(closeness_tensor, 'tensors/closeness_centrality.pt')\n",
        "torch.save(graphlet_four_tensor, 'tensors/graphlet_four.pt')\n",
        "torch.save(graphlet_three_tensor, 'tensors/graphlet_three.pt')\n",
        "\n",
        "torch.save(graph_edge_count, 'tensors/edge_count.pt')\n",
        "torch.save(graph_node_count, 'tensors/node_count.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "FEGOJ8Gg5cAd"
      },
      "outputs": [],
      "source": [
        "degrees_t = torch.load('tensors/degrees.pt')\n",
        "closeness_t = torch.load('tensors/closeness_centrality.pt')\n",
        "betweenness_t = torch.load('tensors/betweenness_centrality.pt')\n",
        "graph_three_t = torch.load('tensors/graphlet_three.pt')\n",
        "graph_four_t = torch.load('tensors/graphlet_four.pt')\n",
        "\n",
        "nodes_c_t = torch.load('tensors/node_count.pt')\n",
        "edges_c_t = torch.load('tensors/edge_count.pt')\n",
        "\n",
        "torch.set_printoptions(precision=17)\n",
        "torch.set_default_dtype(torch.float64)\n",
        "\n",
        "degrees_t = torch.Tensor(degrees_t)\n",
        "closeness_t = torch.Tensor(closeness_t)\n",
        "betweenness_t = torch.Tensor(betweenness_t)\n",
        "nodes_c_t = torch.Tensor(nodes_c_t)\n",
        "edges_c_t = torch.Tensor(edges_c_t)\n",
        "graph_three_t = torch.Tensor(graph_three_t)\n",
        "graph_four_t = torch.Tensor(graph_four_t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sC-1DVZprP-I"
      },
      "outputs": [],
      "source": [
        "# fist stack the features together\n",
        "tensor_new = torch.column_stack((degrees_t, closeness_t, betweenness_t, nodes_c_t, edges_c_t, graph_three_t, graph_four_t))\n",
        "\n",
        "# A data object describing a homogeneous graph\n",
        "new_dataset = Data(x=tensor_new, y=dataset.y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "LXVIEr7CzGIP"
      },
      "outputs": [],
      "source": [
        "# shuffle the whole dataset - this is done after the incorporation is completed\n",
        "\n",
        "# perm = torch.randperm(756) # a tensor containing random numbers\n",
        "# sample_y=torch.index_select(new_dataset.y, 0, perm)\n",
        "# sample_x=torch.index_select(new_dataset.x, 0, perm) # take the corrsponding slice from labels\n",
        "train_fraction = int((756)*0.7)\n",
        "\n",
        "train_x = (new_dataset.x)[:train_fraction]\n",
        "train_y = (new_dataset.y)[:train_fraction]\n",
        "train_y  = train_y.to(torch.float64)\n",
        "# train_dataset = Data(x=train_sliced_permutated_x, y=train_sliced_permutated_y)\n",
        "\n",
        "test_x = (new_dataset.x)[train_fraction:]\n",
        "test_y = (new_dataset.y)[train_fraction:]\n",
        "test_y  = test_y.to(torch.float64)\n",
        "# test_dataset = Data(x=test_sliced_permutation_x, y=test_sliced_permutation_y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "0u44j9VQrHTx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.features[index]\n",
        "        y = self.labels[index]\n",
        "        return {'x': x, 'y': y}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "3w3b0lDdLnMD"
      },
      "outputs": [],
      "source": [
        "# standarization on the features (SCALING)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# TRAIN\n",
        "X_train_standardized = torch.Tensor(scaler.fit_transform(train_x))\n",
        "\n",
        "train_ds = CustomDataset(X_train_standardized, train_y)\n",
        "batch_size = 16\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for batch in train_loader:\n",
        "    features = batch['x'].to(device)\n",
        "    labels = batch['y'].to(device)\n",
        "\n",
        "# TEST\n",
        "X_test_standardized = torch.Tensor(scaler.fit_transform(test_x))\n",
        "test_ds = CustomDataset(X_test_standardized, test_y)\n",
        "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "for batch in test_loader:\n",
        "    features = batch['x'].to(device)\n",
        "    labels = batch['y'].to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "Zra9oSfeEYHA"
      },
      "outputs": [],
      "source": [
        "class LogisticRegression(torch.nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(LogisticRegression, self).__init__()\n",
        "        self.fc1 = torch.nn.Linear(input_size,1)\n",
        "        self.sigmoid = torch.nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.sigmoid(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "9DucgkfwEpez"
      },
      "outputs": [],
      "source": [
        "# input_size = (train_x.shape)[1] # or maybe just 64 - batch size\n",
        "input_size = 7 # num of features\n",
        "model = LogisticRegression(input_size)\n",
        "model.apply(lambda m: torch.nn.init.xavier_uniform_(m.weight) if isinstance(m, torch.nn.Linear) else None)\n",
        "learning_rate = 0.5\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "criterion = torch.nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "a8YBTF8owIIC"
      },
      "outputs": [],
      "source": [
        "def train(data_loader):\n",
        "  for batch in data_loader:\n",
        "      features = batch['x']\n",
        "      labels = batch['y']\n",
        "\n",
        "      # Forward pass\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      probabilities = model(features)\n",
        "      # probabilities = probabilities.squeeze(1)  # Convert y from shape [64] to [64, 1]\n",
        "      # print(probabilities)\n",
        "      # Compute loss\n",
        "      probabilities = probabilities.squeeze(1)  # Convert y from shape [64] to [64, 1]\n",
        "      # print(probabilities)\n",
        "\n",
        "      loss = criterion(probabilities, labels)\n",
        "\n",
        "      # Backward pass and optimization\n",
        "      loss.backward()\n",
        "\n",
        "      # for name, param in model.named_parameters():\n",
        "      #   if param.grad is not None:\n",
        "      #       print(f'Parameter: {name}, Gradient Norm: {param.grad.norm().item()}')\n",
        "      optimizer.step()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "qLAaltfkqWAB"
      },
      "outputs": [],
      "source": [
        "def test_eval(loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    accuracy_lst = []\n",
        "    t_lst = []\n",
        "    prob_lst=[]\n",
        "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
        "        out = model(data['x'])\n",
        "        labels = data['y']\n",
        "\n",
        "        threshold = 0.53\n",
        "        predicted = (out > threshold).float()\n",
        "        predicted = predicted.squeeze(1)\n",
        "        prob_lst.append(out)\n",
        "        t_lst.append(labels)\n",
        "\n",
        "        correct = torch.sum((predicted == labels).float()).item()\n",
        "        total = labels.size(0)\n",
        "\n",
        "        accuracy = 100 * correct / total\n",
        "        accuracy_lst.append(accuracy)\n",
        "    return accuracy_lst, prob_lst, t_lst"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "S8fC5A0qsqNw"
      },
      "outputs": [],
      "source": [
        "for epoch in range(100):\n",
        "    train(train_loader)\n",
        "    test_acc, p, t = test_eval(test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XCPQWK0smu4x",
        "outputId": "e4594b56-d5c2-4705-b2ae-59151877aa1b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "74.58333333333333\n"
          ]
        }
      ],
      "source": [
        "from statistics import mean\n",
        "print(mean(test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5vXGVvsa4yq",
        "outputId": "7d2df7ab-e58b-4060-f6d0-9a09bf68380e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "93.75\n"
          ]
        }
      ],
      "source": [
        "print(max(test_acc))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EP4UXVZKok_W",
        "outputId": "01f8c06a-fa61-4342-f18f-8a26dc2e55e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy of train: 0.9640831758034026\n",
            "Accuracy of test: 0.6431718061674009\n"
          ]
        }
      ],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "train_ds = CustomDataset(X_train_standardized, train_y)\n",
        "test_ds = CustomDataset(X_test_standardized, test_y)\n",
        "tree_classifier = DecisionTreeClassifier()\n",
        "\n",
        "tree_classifier.fit(X_train_standardized, train_y)\n",
        "\n",
        "y_pred = tree_classifier.predict(X_train_standardized)\n",
        "\n",
        "accuracy = accuracy_score(train_y, y_pred)\n",
        "print(f'Accuracy of train: {accuracy}')\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_pred = tree_classifier.predict(X_test_standardized)\n",
        "\n",
        "# Evaluate the accuracy\n",
        "accuracy = accuracy_score(test_y, y_pred)\n",
        "print(f'Accuracy of test: {accuracy}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
